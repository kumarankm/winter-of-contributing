{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_Urllib_Module.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFBxjHKr65hM"
      },
      "source": [
        "# **Python Urllib Module** \n",
        "***urllib — URL handling modules***\n",
        "\n",
        " Python language is used extensively for web programming. When we browse website we use the web address which is also known as *uniform resource locator* (URL). Python has inbuilt materials which can handle the calls to the URL as well as pass the result that comes out of visiting the URL. Which can be done by a module named as **urllib**. We will see the various functions present in this module which help in getting the result from the URL.\n",
        "\n",
        "It uses the *urlopen* function and is able to fetch URLs using a variety of different protocols.Urllib is a package that collects several modules for working with URLs, such as:\n",
        "\n",
        "* urllib.request for opening and reading.\n",
        "* urllib.parse for parsing URLs\n",
        "* urllib.error for the exceptions raised\n",
        "* urllib.robotparser for parsing robot.txt files\n",
        "\n",
        "\n",
        "\n",
        "***Installing urllib:***\n",
        "\n",
        "To install urllib in the python environment, we use the below command using pip.\n",
        "\n",
        "`pip install urllib`\n",
        "\n",
        "\n",
        "Let deep dive in above mentioned modules: \n",
        "* **urllib.request**\n",
        "\n",
        "This module helps to define functions and classes to open URLs (mostly HTTP) and and fetch its content to the python environment.\n",
        "\n",
        "\n",
        "```\n",
        "import urllib.request\n",
        "address = urllib.request.urlopen('https://www.youtube.com/')\n",
        "print(address.read())\n",
        "```\n",
        "This will display the source code of the URL i.e. YouTube. Try it Yourself!!\n",
        "\n",
        "* **urllib.parse**\n",
        "\n",
        "This module helps to define functions to manipulate URLs and their components parts, to build or break them. We can parse the URL to check if it is a valid one or not. It usually focuses on splitting a URL into small components; or joining different URL components into URL strings.\n",
        "\n",
        "\n",
        "```\n",
        "from urllib.parse import * parse_url = urlparse('https://www.geeksforgeeks.org / python-langtons-ant/')\n",
        "print(parse_url)\n",
        "print(\"\\n\")\n",
        "unparse_url = urlunparse(parse_url)\n",
        "print(unparse_url)\n",
        "```\n",
        "**Note**:- The different components of a URL are separated and joined again. Try using some other URL for better understanding.\n",
        "\n",
        "Different other functions of urllib.parse are :\n",
        "\n",
        "**urllib.parse.urlparse:**\tSeparates different components of URL\n",
        "\n",
        "**urllib.parse.urlunparse:**\tJoin different components of URL\n",
        "\n",
        "**urllib.parse.urlsplit:**\tIt is similar to urlparse() but doesn’t split the params\n",
        "\n",
        "**urllib.parse.urlunsplit:**\tCombines the tuple element returned by urlsplit() to form URL\n",
        "\n",
        "**urllib.parse.urldeflag:**\tIf URL contains fragment, then it returns a URL removing the fragment.\n",
        "\n",
        "* **urllib.error**\n",
        "\n",
        "This module defines the classes for exception raised by urllib.request. Whenever there is an error in fetching a URL, this module helps in raising exceptions. The following are the exceptions raised :\n",
        "\n",
        "1. URLError – It is raised for the errors in URLs, or errors while fetching the URL due to connectivity, and has a ‘reason’ property that tells a user the reason of error.\n",
        "2. HTTPError – It is raised for the exotic HTTP errors, such as the authentication request errors. It is a subclass or URLError. Typical errors include ‘404’ (page not found), ‘403’ (request forbidden),\n",
        "and ‘401’ (authentication required).\n",
        "\n",
        "\n",
        "```\n",
        "# URL Error\n",
        "  \n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "  \n",
        "# trying to read the URL but with no internet connectivity\n",
        "try:\n",
        "    x = urllib.request.urlopen('https://www.google.com')\n",
        "    print(x.read())\n",
        "  \n",
        "# Catching the exception generated     \n",
        "except Exception as e :\n",
        "    print(str(e))\n",
        "```\n",
        "output\n",
        "\n",
        "```\n",
        "URL Error: urlopen error [Errno 11001] getaddrinfo failed\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "# HTTP Error\n",
        "  \n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "  \n",
        "# trying to read the URL\n",
        "try:\n",
        "    x = urllib.request.urlopen('https://www.google.com / search?q = test')\n",
        "    print(x.read())\n",
        "  \n",
        "# Catching the exception generated    \n",
        "except Exception as e :\n",
        "    print(str(e))\n",
        "```\n",
        "\n",
        "output\n",
        "\n",
        "```\n",
        "HTTP Error 403: Forbidden\n",
        "```\n",
        "\n",
        "* **urllib.robotparser**\n",
        "\n",
        "This module contains a single class, RobotFileParser. This class answers question about whether or not a particular user can fetch a URL that published robot.txt files. Robots.txt is a text file webmasters create to instruct web robots how to crawl pages on their website. The robot.txt file tells the web scraper about what parts of the server should not be accessed.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# importing robot parser class\n",
        "import urllib.robotparser as rb\n",
        "  \n",
        "bot = rb.RobotFileParser()\n",
        "  \n",
        "# checks where the website's robot.txt file reside\n",
        "x = bot.set_url('https://www.geeksforgeeks.org / robot.txt')\n",
        "print(x)\n",
        "  \n",
        "# reads the files\n",
        "y = bot.read()\n",
        "print(y)\n",
        "  \n",
        "# we can crawl the main site\n",
        "z = bot.can_fetch('*', 'https://www.geeksforgeeks.org/')\n",
        "print(z)\n",
        "  \n",
        "# but can not crawl the disallowed url\n",
        "w = bot.can_fetch('*', 'https://www.geeksforgeeks.org / wp-admin/')\n",
        "print(w)\n",
        "```\n",
        "\n",
        "output\n",
        "\n",
        "```\n",
        "None\n",
        "None\n",
        "True\n",
        "False\n",
        "```\n",
        "\n"
      ]
    }
  ]
}